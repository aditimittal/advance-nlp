{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noKum9ZPRH9u",
        "outputId": "20aca7b4-82c6-45aa-8ac4-d404ac3a452c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prefix embedding tensor size:  torch.Size([3, 10])\n",
            "embedding sum:  tensor([-0.1770, -2.3993, -0.4721,  2.6568,  2.7157, -0.1408, -1.8421, -3.6277,\n",
            "         2.2783,  1.1165], grad_fn=<SumBackward1>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "prefix = 'Bob talked to'\n",
        "\n",
        "# It's 5 word vocabulary and 10-d embeddings\n",
        "embeddings = torch.nn.Embedding(num_embeddings=5, embedding_dim=10)\n",
        "vocab = {'Bob':0, 'talked':1, 'to':2, 'Alice':3, '.':4}\n",
        "\n",
        "# encode our prefix as integer indices that index  into the embeddings matrix. \n",
        "indices = torch.LongTensor([vocab[w] for w in prefix.split()])\n",
        "prefix_embs = embeddings(indices)\n",
        "print('prefix embedding tensor size: ', prefix_embs.size())\n",
        "\n",
        "x = torch.zeros(10)\n",
        "x = torch.sum(prefix_embs, 0)\n",
        "\n",
        "print('embedding sum: ', x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# goal is to produce a probability distribution over the next word, conditioned \n",
        "# on the prefix representation x. This distribution is over the entire \n",
        "# vocabulary. the dimensionality of x is a 10-dimensional vector. first, we need \n",
        "# to **project** this representation down to 5-d. \n",
        "W = torch.rand(10, 5)\n",
        "prod= torch.matmul(W.T,x)\n",
        "\n",
        "probs = torch.nn.functional.softmax(prod)\n",
        "\n",
        "print('probability distribution', probs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLBZytX_Rv-x",
        "outputId": "d849270a-37ee-4b01-f0e3-d63de050cb60"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability distribution tensor([0.0718, 0.0998, 0.1331, 0.6762, 0.0191], grad_fn=<SoftmaxBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-f511e019dcc9>:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  probs = torch.nn.functional.softmax(prod)\n"
          ]
        }
      ]
    }
  ]
}